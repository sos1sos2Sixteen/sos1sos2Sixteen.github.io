<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>VoiceLens Supplementary Materials</title>

  <!-- Latest compiled and minified CSS -->
  <link rel="stylesheet" href="bootstrap-4.5.2-dist/css/bootstrap.min.css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="fontawesome-free-5.15.4-web/css/all.css">
  <!-- mathjax for latex math formulae: -->

  <link rel="dns-prefetch" href="http://cdn.mathjax.org">
  <script type="text/javascript" async
  src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script rel="text/javascript" src="script.js"></script>
  <link rel='stylesheet' href='css/base.css'>

  <style>
    p{
      text-indent:0.5in
    }

    .lead{
      text-indent: 0%;
    }

  </style>
</head>


<body class="py-4" style='font-family: "Roboto", "HelveticaNeue", "Helvetica Neue", Helvetica, Arial, sans-serif;'>
  <div class="container col-md-8">

    <h4 class='display-5'><span class='text-success'>[Site Under construction] Supplementary Samples for</span></h1>
      <h1 class='display-3'> <span class='text-success'>VoiceLens:</span></h1>
      <h1 class="display-4">Controllable Speaker Generation & Editing with Flow</h1>
      <p class='lead'>
        Arxiv: <a href=".">not yet available</a>  </br>
      </p>
      <p class='lead'></p>
      <br />

      <h2 id="authors">Authors</h2>
      <ul class='lead'>
        <li>SHI, Yao (Wuhan University, Duke-Kunshan University)</li>

        <li>DING, Chen (SAMI Bytedance)</li>
        <li>XIA, Rui (SAMI Bytedance)</li>
        <li>HUANG, Chuanzeng (SAMI Bytedance)</li>

        <li>LI, Ming (Duke-Kunshan University, Wuhan University) ······ <a
            href="mailto:ming.li369@dukekunshan.edu.cn">ming.li369@dukekunshan.edu.cn</a></li>
      </ul>

      <div style="visibility: hidden;"><audio id="global-player" controls="controls"><source src="" autoplay /></audio></div>

      <h2>Abstract</h2>
      <p>
        Many multi-speaker speech synthesis and voice conversion systems address speaker variations with an embedding vector. 
        Modeling it directly allows new voices outside of training data to be synthesized. 
        GMM based approaches such as Tacospawn are favored in past literature for this generation task, 
        but have seen their limitations when difficult conditionings are involved. 
      </p>
      <p>

        In this paper, we propose VoiceLens, a semi-supervised flow-based approach to modeling speaker embedding distributions for multi-conditional speaker generation. 
        VoiceLens invertiblely maps speaker embeddings into a combination of independent attributes and residual information. 
        It allows new voices associated with certain attributes to be <b><i>generated</i></b> for existing TTS models, 
        and attributes of known voices meaningfully <b><i>edited</i></b>. 
        We show in this paper, VoiceLens displays unconditional generation capacity that is on par with Tacospawn 
        while attaining higher controllability and flexibility when conditioned. 
        In addition, we show synthesizing less noisy speech from known noisy speakers without re-training the TTS system is possible via solely editing their embeddings with a SNR conditioned VoiceLens model. 
      </p>



      <br />
      <h2>Method Overview</h2>
      <p>
        Figure 1 below is an illustrated overview of the proposed VoiceLens workflow. 
        Given a trained Multispeaker TTS system, a set of known speaker embedding vectors (\(\mathbf{e}\)) and the speakers' partially labeled attributes (\(\mathbf{y}\))
        (such as gender, age-group or general SNR level), 
        a Normalizing Flow is trained in a semi-supervised manner to model the conditional distribution \(p(\mathbf{e}|\mathbf{y})\) of the speaker embedding conditioning on the modeled attributes. 
        Multiple attributes are handled by partitioning the flow's base variable \(\mathbf{z}\) into multiple disjoint subspaces (assuming independence). 
        Once trained, new speaker embeddings with desired attributes can be generated by first sampling from the appropriated prior distributions on \(\mathbf{z}\), 
        then transforming the samples into \(\mathbf{e}\) by inversing the flow. 
        Attributes of a known speaker embedding could be modified by first transforming its embedding into \(\mathbf{z}\), 
        swapping in new values according to the prior distribution, then inversing the edited \(\mathbf{z}\) into \(\mathbf{e}\) with flow.
      </p>

      <div class="row">
        <div class="col-md-2"></div>
        <div class="col-md-8">
          <figure class='figure'>
            <img src="assets/img/diagrams-overview.png" class="figure-img img-fluid rounded" />
            <figcaption class='figure-caption'>
              Figure 1. Overview of VoiceLens Workflow</i>
            </figcaption>
          </figure>
        </div>
      </div>


      <br/>
      <h2>Experimental Setup</h2>
      <p>
        We used a Multispeaker VITS with speaker Look-Up-Table as our experimental TTS system. 
        It was trained on DidiSpeech-2, a Mandarin Corpus containing around 1500 speakers. 
        <i>The TTS system was trained once and remains un-modified throughout our experiments.</i>
        We modeled the speaker embedding distribution conditioning on the multi-label 
        <b>gender</b> (female/male), <b>age-group</b> (child/adult) and <b>SNR</b> level (continuously distributed within [20,60] dB) 
        with the proposed VoiceLens method.
      </p>

      <br/>
      <h2>Demo 1: Unconditional Generation</h2>
      <p>
        The following table presents, for each evaluation sentence, three synthesized utterances generated by the same Multi-speaker TTS system.
        They differ only in the synthesizer's input speaker embedding. For <b>parallel synthesis</b>, the speaker embedding of the original speaker 
        to which the evaluation sentence belongs is provided as input. (As the case in standard multi-speaker TTS). For <b>VoiceLens generated</b>, 
        the input embedding is unconditionally sampled from the trained VoiceLens model, representing a newly generated voice. For <b>nearest known</b>, 
        the input embedding is selected as the embedding of the known speaker whose voice is the closest to the generated voice in terms of speaker verification 
        cosine distance.

      </p>
      <table class="table table-hover" style="text-align: center;">
        <thead>
          <tr>
            <td>Id</td>
            <td>parallel synthesis</td>
            <td>VoiceLens generated</td>
            <td>nearest known</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1</td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
          </tr> 
          <tr>
            <td>2</td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
          </tr> 
        </tbody>
      </table>

      <br/>
      <h2>Demo 2: Conditional Generation</h2>
      <p>
        The following table presents a series of conditional generation results obtained from the proposed model.
        For each row, a (gender-age) condition is specified before sampling in \(\mathbf{z}\). After an instance of \(\mathbf{z}\)
        is sampled, the dimensions related to SNR modeling (denoted by \(z_{snr}\) here) are set as values from the range(20, 60, 10), forming the columns in table.

        We present synthesized utterances for voices generated under the above stated conditions, and report SNRs estimated from these samples. It can be
        observed that the actual SNR measured <i>post-hoc</i> closely follows the controlling conditions (\(z_{snr}\)) set before generation.

      </p>
      <p class="lead">
        <mark>Left click on the mel-spectrogram images to play samples!</mark>
      </p>
      <table class="table table-condensed table-hover">
        <tr>
          <td>condition</td>
          <td>\(z_{snr}\)</td>
          <td>20</td>
          <td>30</td>
          <td>40</td>
          <td>50</td>
          <td>60</td>
        </tr>


        <tr>
          <td rowspan="2">female child</td>
          <td>SNR/dB</td>
          <td>23.501</td>
          <td>31.815</td>
          <td>36.330</td>
          <td>42.908</td>
          <td>53.411</td>
        </tr>
        <tr>
          <td>Melspec</td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-20.PNG" onclick="clk_img()"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-30.PNG"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-40.PNG"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-50.PNG"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-60.PNG"></td>
        </tr>
        

        <tr>
          <td rowspan="2">male child</td>
          <td>SNR/dB</td>
          <td>23.501</td>
          <td>31.815</td>
          <td>36.330</td>
          <td>42.908</td>
          <td>53.411</td>
        </tr>
        <tr>
          <td>Melspec</td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-20.PNG" onclick="clk_img()"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-30.PNG"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-40.PNG"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-50.PNG"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-60.PNG"></td>
        </tr>

        <tr>
          <td rowspan="2">female adult</td>
          <td>SNR/dB</td>
          <td>23.501</td>
          <td>31.815</td>
          <td>36.330</td>
          <td>42.908</td>
          <td>53.411</td>
        </tr>
        <tr>
          <td>Melspec</td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-20.PNG" onclick="clk_img()"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-30.PNG"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-40.PNG"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-50.PNG"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-60.PNG"></td>
        </tr>

        <tr>
          <td rowspan="2">male adult</td>
          <td>SNR/dB</td>
          <td>23.501</td>
          <td>31.815</td>
          <td>36.330</td>
          <td>42.908</td>
          <td>53.411</td>
        </tr>
        <tr>
          <td>Melspec</td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-20.PNG" onclick="clk_img()"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-30.PNG"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-40.PNG"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-50.PNG"></td>
          <td><img class="figure-img img-fluid " src="assets/img/conditioned/snr-fc-60.PNG"></td>
        </tr>
      </table>
    
      <br/>
      <h2>Demo 3: Known Speaker Editing</h2>

      <br/>
      <h3>3.1: Flipping Categorical Attributes (gender-swapping)</h3>      

      <p>
        The following table presents 
      </p>
      <p>
        The following table presents synthesis results based on real and edited speaker embeddings as descirbed by Figure 1.
        For each row, we choose a evaluation sentence of a known speaker, examine the recording and a parallel synthesis result. 

      </p>
      <table class="table table-hover" style="text-align: center;">
        <thead>
          <tr>
            <td>case</td>
            <td>recording</td>
            <td>synthesized</td>
            <td>flip gender</td>
            <td>flip age-group</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1</td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
          </tr>
          
          <tr>
            <td>2</td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
            <td><button type="button" class="btn btn-default" onclick="console.log('btn')"><i class="far fa-lg fa-play-circle"></i></button></td>
          </tr>
        </tbody>


      </table>


      <br/>
      <h3>3.2: Adjusting Continuous Attributes (de-noising)</h3>


      <br/>
      <h3>3.3: More De-noising Samples</h3>

  </div>


</body>



<!-- Latest compiled and minified JavaScript -->
<!-- <script src="bootstrap-4.5.2-dist/js/bootstrap.bundle.js"></script> -->

</html>